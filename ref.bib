
@article{c_comprehensive_2024,
	title = {A {Comprehensive} {Study} of {Walmart} {Sales} {Predictions} {Using} {Time} {Series} {Analysis}},
	volume = {20},
	issn = {2456-477X},
	url = {https://journalarjom.com/index.php/ARJOM/article/view/809},
	doi = {10.9734/arjom/2024/v20i7809},
	abstract = {This article presents a comprehensive study of sales predictions using time series analysis, focusing on a case study of Walmart sales data. The aim of this study is to evaluate the effectiveness of various time series forecasting techniques in predicting weekly sales data for Walmart stores. Leveraging a dataset from Kaggle comprising weekly sales data from various Walmart stores around the United States, this study explores the effectiveness of time series analysis in forecasting future sales trends. Various time series analysis techniques, including Auto Regressive Integrated Moving Average (ARIMA), Seasonal Auto Regressive Integrated Moving Average (SARIMA), Prophet, Exponential Smoothing, and Gaussian Processes, are applied to model and forecast Walmart sales data. By comparing the performance of these models, the study seeks to identify the most accurate and reliable methods for forecasting retail sales, thereby providing valuable insights for improving sales predictions in the retail sector. The study includes an extensive exploratory data analysis (EDA) phase to preprocess the data, detect outliers, and visualize sales trends over time. Additionally, the article discusses the partitioning of data into training and testing sets for model evaluation. Performance metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are utilized to compare the accuracy of different time series models.
The results indicate that Gaussian Processes outperform other models in terms of accuracy, with an RMSE of 34,116.09 and an MAE of 25,495.72, significantly lower than the other models evaluated. For comparison, ARIMA and SARIMA models both yielded an RMSE of 555,502.2 and an MAE of 462,767.3, while the Prophet model showed an RMSE of 567,509.2 and an MAE of 474,990.8. Exponential Smoothing also performed well with an RMSE of 555,081.7 and an MAE of 464,110.5. These findings suggest the potential of Gaussian Processes for accurate sales forecasting. However, the study also highlights the strengths and weaknesses of each forecasting methodology, emphasizing the need for further research to refine existing techniques and explore novel modeling approaches. Overall, this study contributes to the understanding of time series analysis in retail sales forecasting and provides insights for improving future forecasting endeavors.},
	language = {en},
	number = {7},
	urldate = {2025-12-22},
	journal = {Asian Research Journal of Mathematics},
	author = {C., Cyril Neba and F., Gerard Shu and Nsuh, Gillian and A., Philip Amouda and F., Adrian Neba and Webnda, F. and Ikpe, Victory and Orelaja, Adeyinka and Sylla, Nabintou Anissia},
	month = jun,
	year = {2024},
	pages = {9--30},
	file = {PDF:/Users/malakkhan/Zotero/storage/NZ4UNIH4/C. et al. - 2024 - A Comprehensive Study of Walmart Sales Predictions Using Time Series Analysis.pdf:application/pdf},
}

@article{young_dynamic_1999,
	title = {Dynamic harmonic regression},
	volume = {18},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {0277-6693, 1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-131X(199911)18:6<369::AID-FOR748>3.0.CO;2-K},
	doi = {10.1002/(SICI)1099-131X(199911)18:6<369::AID-FOR748>3.0.CO;2-K},
	abstract = {This paper describes in detail a flexible approach to nonstationary time series analysis based on a Dynamic Harmonic Regression (DHR) model of the Unobserved Components (UC) type, formulated within a stochastic state space setting. The model is particularly useful for adaptive seasonal adjustment, signal extraction and interpolation over gaps, as well as forecasting or backcasting. The Kalman Filter and Fixed Interval Smoothing algorithms are exploited for estimating the various components, with the Noise Variance Ratio and other hyperparameters in the stochastic state space model estimated by a novel optimization method in the frequency domain. Unlike other approaches of this general type, which normally exploit Maximum Likelihood methods, this optimization procedure is based on a cost function defined in terms of the difference between the logarithmic pseudo-spectrum of the DHR model and the logarithmic autoregressive spectrum of the time series. The cost function not only seems to yield improved convergence characteristics when compared with the alternative ML cost function, but it also has much reduced numerical requirements.},
	language = {en},
	number = {6},
	urldate = {2025-12-22},
	journal = {Journal of Forecasting},
	author = {Young, Peter C. and Pedregal, Diego J. and Tych, Wlodek},
	month = nov,
	year = {1999},
	pages = {369--394},
}

@article{hyndman_forecasting_nodate,
	title = {Forecasting: principles and practice},
	language = {en},
	journal = {Regression models},
	author = {Hyndman, Rob J},
	file = {PDF:/Users/malakkhan/Zotero/storage/H6RX4WJM/Hyndman - Forecasting principles and practice.pdf:application/pdf},
}

@book{noauthor_105_nodate,
	title = {10.5 {Dynamic} harmonic regression {\textbar} {Forecasting}: {Principles} and {Practice} (3rd ed)},
	shorttitle = {10.5 {Dynamic} harmonic regression {\textbar} {Forecasting}},
	url = {https://otexts.com/fpp3/dhr.html},
	abstract = {3rd edition},
	urldate = {2025-12-22},
	file = {Snapshot:/Users/malakkhan/Zotero/storage/VNQ3EE4G/dhr.html:text/html},
}

@book{noauthor_95_nodate,
	title = {9.5 {Dynamic} harmonic regression {\textbar} {Forecasting}: {Principles} and {Practice} (2nd ed)},
	shorttitle = {9.5 {Dynamic} harmonic regression {\textbar} {Forecasting}},
	url = {https://otexts.com/fpp2/dhr.html},
	abstract = {2nd edition},
	urldate = {2025-12-22},
	file = {Snapshot:/Users/malakkhan/Zotero/storage/9R9JF7LE/dhr.html:text/html},
}

@book{noauthor_121_nodate,
	title = {12.1 {Weekly}, daily and sub-daily data {\textbar} {Forecasting}: {Principles} and {Practice} (2nd ed)},
	shorttitle = {12.1 {Weekly}, daily and sub-daily data {\textbar} {Forecasting}},
	url = {https://otexts.com/fpp2/weekly.html},
	abstract = {2nd edition},
	urldate = {2025-12-22},
	file = {Snapshot:/Users/malakkhan/Zotero/storage/LLDZNWVY/weekly.html:text/html},
}

@book{noauthor_132_nodate,
	title = {13.2 {Time} series of counts {\textbar} {Forecasting}: {Principles} and {Practice} (3rd ed)},
	shorttitle = {13.2 {Time} series of counts {\textbar} {Forecasting}},
	url = {https://otexts.com/fpp3/counts.html},
	abstract = {3rd edition},
	urldate = {2025-12-22},
	file = {Snapshot:/Users/malakkhan/Zotero/storage/UA87CJ3H/counts.html:text/html},
}

@article{syntetos_bias_2001,
	series = {Tenth {International} {Symposium} on {Inventories}},
	title = {On the bias of intermittent demand estimates},
	volume = {71},
	issn = {0925-5273},
	url = {https://www.sciencedirect.com/science/article/pii/S0925527300001432},
	doi = {10.1016/S0925-5273(00)00143-2},
	abstract = {Forecasting and inventory control for intermittent demand items has been a major problem in the manufacturing and supply environment. Croston (Operational Research Quarterly 23 (1972) 289), proposed a method according to which intermittent demand estimates can be built from constituent events. Croston's method has been reported to be a robust method but has shown more modest benefits in forecasting accuracy than expected. In this research, one of the causes of this unexpected performance has been identified, as a first step towards improving Croston's method. Certain limitations are identified in Croston's approach and a correction in his derivation of the expected estimate of demand per time period is presented. In addition, a modification to his method that gives approximately unbiased demand per period estimates is introduced. All the conclusions are confirmed by means of an extended simulation experiment where Croston's and Revised Croston's methods are compared. The forecasting accuracy comparison corresponds to a situation of an inventory control system employing a re-order interval or product group review.},
	number = {1},
	urldate = {2025-12-22},
	journal = {International Journal of Production Economics},
	author = {Syntetos, A. A and Boylan, J. E},
	month = may,
	year = {2001},
	keywords = {Forecasting, Intermittent demand, Inventory},
	pages = {457--466},
	file = {ScienceDirect Snapshot:/Users/malakkhan/Zotero/storage/YBBCV5XS/S0925527300001432.html:text/html},
}

@article{hyndman_stochastic_2005,
	title = {Stochastic models underlying {Croston}'s method for intermittent demand forecasting},
	volume = {24},
	abstract = {Croston's method is widely used to predict inventory demand when it is intermittent. However, it is an ad hoc method with no properly formulated underlying stochastic model. In this paper, we explore possible models underlying Croston's method and three related methods, and we show that any underlying model will be inconsistent with the properties of intermittent demand data. However, we find that the point forecasts and prediction intervals based on such underlying models may still be useful. Copyright © 2005 John Wiley \& Sons, Ltd.},
	journal = {Journal of Forecasting},
	author = {Hyndman, Rob and Shenstone, Lydia},
	month = feb,
	year = {2005},
	pages = {389--402},
	file = {Full Text PDF:/Users/malakkhan/Zotero/storage/AXBF6KG3/Hyndman and Shenstone - 2005 - Stochastic models underlying Croston's method for intermittent demand forecasting.pdf:application/pdf},
}

@article{croston_forecasting_1972,
	title = {Forecasting and {Stock} {Control} for {Intermittent} {Demands}},
	volume = {23},
	issn = {1476-9360},
	url = {https://doi.org/10.1057/jors.1972.50},
	doi = {10.1057/jors.1972.50},
	abstract = {Exponential smoothing is frequently used for the forecasts in stock control systems. The analysis given shows that intermittent demands almost always produce inappropriate stock levels. Demand for constant quantities at fixed intervals may generate stock levels of up to double the quantity really needed. A method of overcoming these difficulties is described, using separate estimates of the size of demand, and of the demand frequency. The rules for setting the safety stock levels have also to be adjusted before consistent protection can be obtained against being out of stock.},
	language = {en},
	number = {3},
	urldate = {2025-12-25},
	journal = {Journal of the Operational Research Society},
	author = {Croston, J. D.},
	month = sep,
	year = {1972},
	pages = {289--303},
}

@inproceedings{ke_lightgbm_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	volume = {30},
	shorttitle = {{LightGBM}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
	urldate = {2025-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	year = {2017},
	file = {Full Text PDF:/Users/malakkhan/Zotero/storage/9D9WCM2P/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Decision Tree.pdf:application/pdf},
}

@misc{oreshkin_n-beats_2020,
	title = {N-{BEATS}: {Neural} basis expansion analysis for interpretable time series forecasting},
	shorttitle = {N-{BEATS}},
	url = {http://arxiv.org/abs/1905.10437},
	doi = {10.48550/arXiv.1905.10437},
	abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
	urldate = {2025-12-25},
	publisher = {arXiv},
	author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
	month = feb,
	year = {2020},
	note = {arXiv:1905.10437 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/malakkhan/Zotero/storage/WLRIU7WX/Oreshkin et al. - 2020 - N-BEATS Neural basis expansion analysis for interpretable time series forecasting.pdf:application/pdf;Snapshot:/Users/malakkhan/Zotero/storage/URNKTZLC/1905.html:text/html},
}

@misc{garza_timegpt-1_2024,
	title = {{TimeGPT}-1},
	url = {http://arxiv.org/abs/2310.03589},
	doi = {10.48550/arXiv.2310.03589},
	abstract = {In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.},
	urldate = {2025-12-25},
	publisher = {arXiv},
	author = {Garza, Azul and Challu, Cristian and Mergenthaler-Canseco, Max},
	month = may,
	year = {2024},
	note = {arXiv:2310.03589 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications},
	file = {Preprint PDF:/Users/malakkhan/Zotero/storage/ZMGCVER7/Garza et al. - 2024 - TimeGPT-1.pdf:application/pdf;Snapshot:/Users/malakkhan/Zotero/storage/KTNXZVI9/2310.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2025-12-25},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}
