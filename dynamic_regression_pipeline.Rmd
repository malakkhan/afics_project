---
title: "Sales Data Analysis"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Load Data

We use `read_csv` for parsing.

```{r load_libraries_data}
library(fpp3)
library(tidyverse)
library(lubridate)
library(stringr)

sales_raw <- read_csv("data/sales_train_validation_afcs2025.csv")
calendar_raw <- read_csv("data/calendar_afcs2025.csv")
prices_raw <- read_csv("data/sell_prices_afcs2025.csv")
```

## 2. Process Calendar

The user assumes `d_1` corresponds to 2011-01-29. We ensure the calendar dates are parsed correctly. We also create a 'd' column to match with sales columns (`d_1`, `d_2`, ...).

```{r process_calendar}
calendar <- calendar_raw %>%
  mutate(date = mdy(date)) %>%
  # Create a 'd' column to match with sales columns (d_1, d_2, ...)
  mutate(d = paste0("d_", row_number()))
```

### Identify unique events

```{r identify_events}
event_types <- c(unique(calendar_raw$event_name_1), unique(calendar_raw$event_name_2))
event_types <- event_types[!is.na(event_types)]
event_types <- unique(event_types)
```

### Checking for any inconsistencies

The user asked to check for changes in spaces or lowercase/uppercase. We can normalize them to lower case and check if we have fewer unique values.

```{r check_inconsistencies}
normalized_events <- tolower(str_replace_all(event_types, "[^[:alnum:]]", ""))
if(length(unique(normalized_events)) < length(event_types)) {
  warning("Same events found with different casing or formatting. Please inspect.")
}
```

### Create Event Columns

Ensure calendar is sorted by date for `lead()` to work correctly. Create binary columns for each event.
Logic: 1 if event matches `event_name_1` OR `event_name_2`, OR if it is one of the 3 days BEFORE the event (using lead checks on future dates).

```{r create_event_flags}
# Ensure calendar is sorted by date for lead() to work correctly
calendar <- calendar %>% arrange(date)

# Create binary columns for each event
for(evt in event_types) {
  # Create a clean column name (remove spaces and special chars)
  col_name <- str_replace_all(evt, "[^[:alnum:]]", "")
  
  # Add binary column
  calendar <- calendar %>%
    mutate(
      is_event_today = coalesce(event_name_1 == evt, FALSE) | coalesce(event_name_2 == evt, FALSE),
      !!col_name := as.integer(
        is_event_today |
        lead(is_event_today, 1, default = FALSE) |
        lead(is_event_today, 2, default = FALSE) |
        lead(is_event_today, 3, default = FALSE)
      )
    ) %>%
    select(-is_event_today)
}
```

## 3. Process Sales Data

Pivot from wide to long format to create a time series structure.

```{r process_sales}
sales_long <- sales_raw %>%
  pivot_longer(
    cols = starts_with("d_"),
    names_to = "d",
    values_to = "sales_volume"
  )
```

## 4. Extract Item and Store IDs from the 'id' column

Instruction: "CORRECT ITEMID (which can be isolated from the first 11 digits of the id column...)"
We also need `store_id` to join with prices.

```{r extract_ids}
sales_long <- sales_long %>%
  mutate(
    # Extract first 11 characters for item_id
    item_id = str_sub(id, 1, 11),
    # Extract store_id. 
    # Logic: The id is typically {item_id}_{store_id}_validation
    # We take the substring after the item_id (from char 13) and remove '_validation'
    store_id_temp = str_sub(id, 13, nchar(id)),
    store_id = str_remove(store_id_temp, "_validation")
  ) %>%
  select(-store_id_temp)
```

## 5. Merge with Calendar Information

Join by 'd' identifier.

```{r merge_calendar}
sales_with_calendar <- sales_long %>%
  left_join(calendar, by = "d")
```

## 6. Merge with Sell Prices

Join using `wm_yr_wk`, `store_id`, and `item_id`.

```{r merge_prices}
final_data <- sales_with_calendar %>%
  left_join(prices_raw, by = c("store_id", "item_id", "wm_yr_wk"))
```

## 7. Convert to tsibble

The output should be one tsibble containing day-granularity time series.
We use `id` as the key to distinguish each product time series.
We use `date` as the time index.

```{r create_tsibble}
product_tsibble <- final_data %>%
  # Ensure date is sorted (tsibble expects sorted index usually, though as_tsibble handles it)
  arrange(id, date) %>%
  as_tsibble(key = id, index = date)

# Cleanup large intermediate objects to free memory
rm(sales_raw, calendar_raw, prices_raw, sales_long, sales_with_calendar, final_data)
gc()

# Output the final tsibble
product_tsibble
```

## 8. Visualization

The following graph displays the sum of product sales over the full available time window.

```{r plot_sales_analysis, fig.width=12, fig.height=3}
library(patchwork) # Required for arranging plots side-by-side

# 1. Daily Aggregation & Analysis
# -------------------------------
daily_sales <- product_tsibble %>%
  index_by(date) %>%
  summarise(total_sales = sum(sales_volume, na.rm = TRUE))

# Plot: Daily Sales Over Time
daily_sales %>%
  autoplot(total_sales) +
  labs(
    title = "Total Daily Sales Volume Over Time",
    y = "Sum of Sales",
    x = "Date"
  ) +
  theme_minimal()

# Plot: Daily ACF and PACF (14 lags / 2 weeks)
p1_daily <- daily_sales %>%
  ACF(total_sales, lag_max = 14) %>%
  autoplot() +
  labs(title = "Daily ACF (14 Lags)") +
  theme_minimal()

p2_daily <- daily_sales %>%
  PACF(total_sales, lag_max = 14) %>%
  autoplot() +
  labs(title = "Daily PACF (14 Lags)") +
  theme_minimal()

# Display side-by-side
p1_daily + p2_daily

# 2. Weekly Aggregation & Analysis
# --------------------------------
weekly_sales <- daily_sales %>%
  index_by(week = yearweek(date)) %>%
  summarise(total_sales = sum(total_sales, na.rm = TRUE))

# Plot: Weekly Sums
weekly_sales %>%
  autoplot(total_sales) +
  labs(
    title = "Total Weekly Sales Volume",
    y = "Weekly Sum of Sales",
    x = "Week"
  ) +
  theme_minimal()

# Plot: Weekly ACF and PACF (8 lags / 2 months)
p1_weekly <- weekly_sales %>%
  ACF(total_sales, lag_max = 8) %>%
  autoplot() +
  labs(title = "Weekly ACF (8 Lags)") +
  theme_minimal()

p2_weekly <- weekly_sales %>%
  PACF(total_sales, lag_max = 8) %>%
  autoplot() +
  labs(title = "Weekly PACF (8 Lags)") +
  theme_minimal()

p1_weekly + p2_weekly

# 3. Monthly Aggregation & Analysis
# ---------------------------------
monthly_sales <- daily_sales %>%
  index_by(month = yearmonth(date)) %>%
  summarise(total_sales = sum(total_sales, na.rm = TRUE))

# Plot: Monthly Sums
monthly_sales %>%
  autoplot(total_sales) +
  labs(
    title = "Total Monthly Sales Volume",
    y = "Monthly Sum of Sales",
    x = "Month"
  ) +
  theme_minimal()

# Plot: Monthly ACF and PACF (24 lags / 2 years)
p1_monthly <- monthly_sales %>%
  ACF(total_sales, lag_max = 24) %>%
  autoplot() +
  labs(title = "Monthly ACF (24 Lags)") +
  theme_minimal()

p2_monthly <- monthly_sales %>%
  PACF(total_sales, lag_max = 24) %>%
  autoplot() +
  labs(title = "Monthly PACF (24 Lags)") +
  theme_minimal()

p1_monthly + p2_monthly
```


## 9. Predictor Selection and Exploratory Analysis

According to FPP3 Chapter 7, the first step in building a regression model is ensuring our predictors have a relationship with the forecast variable. Since we have thousands of products, performing this diagnosis on every single time series individually is impractical. Instead, we either select representative products to explore the efficacy of our potential predictors or compare the predictors to the sum total sales across products. We inspect the relationship between Sales Volume and Price. Economic theory suggests a negative correlation (as price goes up, sales go down), but we should verify this in the data. We select a random sample of **10 products** to explore the relationship between Sales Volume and Price across this sample. Economic theory suggests a negative correlation (as price goes up, sales go down), and we verify this pattern across multiple items below:

```{r sample_10_products, fig.width=15, fig.height=15}
library(patchwork)

# 1. Select 10 random products
set.seed(123) # Set seed for reproducibility
all_ids <- unique(product_tsibble$id)
sample_ids <- sample(all_ids, 10)

# 2. Define a helper function to plot "Sales vs Price" for a given ID
plot_price_vs_sales <- function(target_id) {
  product_tsibble %>%
    filter(id == target_id) %>%
    ggplot(aes(x = sell_price, y = sales_volume)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    labs(
      title = paste("Item:", target_id),
      x = "Sell Price", 
      y = "Sales Volume"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(size = 10, face = "bold"))
}

# 3. Generate plots for all 10 IDs
plot_list <- map(sample_ids, plot_price_vs_sales)

# 4. Arrange in a grid: 5 rows, 2 columns
wrap_plots(plot_list, ncol = 2)
```

Next, we look at Categorical Predictors like snap_TX and events:

```{r plot_snap_comparison}
# Aggregate total sales per day
daily_sales_agg <- product_tsibble %>%
  index_by(date) %>%
  summarise(total_sales = sum(sales_volume, na.rm = TRUE)) %>%
  # Join with calendar to get the snap_TX column
  left_join(calendar, by = "date")

# Create comparative boxplot
daily_sales_agg %>%
  mutate(snap_TX = factor(snap_TX, levels = c(0, 1), labels = c("No SNAP", "SNAP Active"))) %>%
  ggplot(aes(x = snap_TX, y = total_sales, fill = snap_TX)) +
  geom_boxplot() +
  labs(
    title = "Total Sales Distribution: SNAP vs Non-SNAP Days",
    x = "Is SNAP Active?",
    y = "Total Daily Sales Volume"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```



```{r plot_event_impact, fig.width=15, fig.height=60}
# 1. Aggregate to Daily Global Sales
# ----------------------------------
daily_sales_agg <- product_tsibble %>%
  index_by(date) %>%
  summarise(total_sales = sum(sales_volume, na.rm = TRUE))

# 2. Join with Calendar to retrieve Event Flags
# ---------------------------------------------
# We join the daily sales with the calendar to access the binary event columns created earlier.
analysis_data <- daily_sales_agg %>%
  left_join(calendar, by = "date")

# 3. Define the list of events to analyze (as requested)
# -----------------------------------------------------
# We list them in lowercase to match case-insensitively against the actual columns.
target_events_list <- c(
  "superbowl", "valentinesday", "presidentsday", "lentstart", "lentweek2", 
  "stpatricksday", "purimend", "orthodoxeaster", "pesachend", "cincodemayo", 
  "mothersday", "memorialday", "nbafinalsstart", "nbafinalsend", "fathersday", 
  "independenceday", "ramadanstarts", "eidalfitr", "laborday", "columbusday", 
  "halloween", "eidaladha", "veteransday", "thanksgiving", "christmas", 
  "chanukahend", "newyear", "orthodoxchristmas", "martinlutherkingday", "easter",
  "snap_TX"
)

# 4. Process Data for Plotting
# ----------------------------
plot_data <- analysis_data %>%
  # Select date, sales, and columns that match our target list (ignoring case)
  select(date, total_sales, matches(paste0("^(", paste(target_events_list, collapse="|"), ")$"), ignore.case = TRUE)) %>%
  # Pivot longer to stack all events into two columns: 'event_name' and 'is_active'
  pivot_longer(
    cols = -c(date, total_sales), 
    names_to = "event_name", 
    values_to = "is_active"
  ) %>%
  # Convert 1/0 to meaningful Factors ("Yes"/"No")
  mutate(
    is_active = factor(is_active, levels = c(0, 1), labels = c("No", "Yes")),
    # Format labels to "CamelCase" / Title Case for better readability
    event_label = str_to_title(event_name),
    # Manual fix for 'snap_TX' if str_to_title messes it up (optional polish)
    event_label = ifelse(event_name == "snap_TX", "Snap TX", event_label)
  )

# 5. Generate Boxplots
# --------------------
plot_data %>%
  ggplot(aes(x = is_active, y = total_sales, fill = is_active)) +
  geom_boxplot(outlier.alpha = 0.3, outlier.size = 1) +
  # Facet wrap creates the "numerous box plots" layout
  facet_wrap(~ event_label, scales = "free_y", ncol = 5) + 
  labs(
    title = "Impact of Events on Total Daily Sales Volume",
    subtitle = "Comparison of Daily Sales: Days with Event (Yes) vs. Days without (No)",
    x = "Is Event Active?",
    y = "Total Daily Sales Volume"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    strip.text = element_text(face = "bold", size = 9)
  )
```

```{r compute_event_impact}
# 1. Aggregate to Daily Global Sales
# ----------------------------------
daily_sales_agg <- product_tsibble %>%
  index_by(date) %>%
  summarise(total_sales = sum(sales_volume, na.rm = TRUE)) %>%
  left_join(calendar, by = "date")

# 2. Define the list of events to analyze
# ---------------------------------------
target_events_list <- c(
  "superbowl", "valentinesday", "presidentsday", "lentstart", "lentweek2", 
  "stpatricksday", "purimend", "orthodoxeaster", "pesachend", "cincodemayo", 
  "mothersday", "memorialday", "nbafinalsstart", "nbafinalsend", "fathersday", 
  "independenceday", "ramadanstarts", "eidalfitr", "laborday", "columbusday", 
  "halloween", "eidaladha", "veteransday", "thanksgiving", "christmas", 
  "chanukahend", "newyear", "orthodoxchristmas", "martinlutherkingday", "easter",
  "snap_TX"
)
# 3. Compute Averages and Differences
# -----------------------------------
impact_table <- daily_sales_agg %>%
  # IMPORTANT: Convert to a standard tibble to drop the date index during summarise
  as_tibble() %>% 
  select(total_sales, matches(paste0("^(", paste(target_events_list, collapse="|"), ")$"), ignore.case = TRUE)) %>%
  # Pivot to long format
  pivot_longer(
    cols = -total_sales,
    names_to = "event_name",
    values_to = "is_active"
  ) %>%
  # Now this will group globally across all dates
  group_by(event_name, is_active) %>%
  summarise(avg_sales = mean(total_sales, na.rm = TRUE), .groups = "drop") %>%
  # Reshape to have columns for "No Event" (0) and "Event" (1)
  pivot_wider(
    names_from = is_active,
    values_from = avg_sales,
    names_prefix = "active_"
  ) %>%
  # Calculate the Difference
  mutate(
    avg_sales_without = active_0,
    avg_sales_with = active_1,
    difference = avg_sales_with - avg_sales_without
  ) %>%
  select(event_name, avg_sales_with, avg_sales_without, difference) %>%
  arrange(desc(difference))

# 4. Display the Table
# --------------------
# Using knitr::kable for a clean table output in RMarkdown
knitr::kable(impact_table, caption = "Average Sales Impact by Event (Sorted by Positive Effect)")
```


```{r event_hypothesis_testing}
# 1. Aggregate to Daily Global Sales & Join Calendar
# --------------------------------------------------
daily_sales_agg <- product_tsibble %>%
  index_by(date) %>%
  summarise(total_sales = sum(sales_volume, na.rm = TRUE)) %>%
  left_join(calendar, by = "date")

# 2. Define the list of events
# ----------------------------
target_events_list <- c(
  "superbowl", "valentinesday", "presidentsday", "lentstart", "lentweek2", 
  "stpatricksday", "purimend", "orthodoxeaster", "pesachend", "cincodemayo", 
  "mothersday", "memorialday", "nbafinalsstart", "nbafinalsend", "fathersday", 
  "independenceday", "ramadanstarts", "eidalfitr", "laborday", "columbusday", 
  "halloween", "eidaladha", "veteransday", "thanksgiving", "christmas", 
  "chanukahend", "newyear", "orthodoxchristmas", "martinlutherkingday", "easter",
  "snap_TX"
)

# 3. Perform T-Tests for each event
# ---------------------------------
# We iterate over each event name, find the matching column, and run a t-test.
hypothesis_results <- map_dfr(target_events_list, function(evt_name) {
  
  # Identify the actual column name in the dataframe (handling case insensitivity)
  # We look for a column that matches the event name exactly (anchored)
  col_match <- names(daily_sales_agg)[str_detect(names(daily_sales_agg), regex(paste0("^", evt_name, "$"), ignore_case = TRUE))]
  
  # Safety check: if column not found or multiple matches, skip or pick first
  if(length(col_match) == 0) return(NULL)
  col_name <- col_match[1]
  
  # Extract relevant data
  test_data <- daily_sales_agg %>%
    as_tibble() %>%
    select(total_sales, is_active = !!sym(col_name))
  
  # Ensure we have data for both groups (0 and 1)
  # A t-test will fail if one group is empty or has only 1 observation sometimes
  counts <- table(test_data$is_active)
  if(length(counts) < 2 || min(counts) < 2) {
    return(tibble(
      event_name = evt_name,
      t_statistic = NA,
      p_value = NA,
      mean_with = NA,
      mean_without = NA,
      note = "Insufficient data"
    ))
  }
  
  # Run Two-Sided T-Test (assuming unequal variances / Welch's t-test)
  # H0: mean(sales|active) == mean(sales|inactive)
  # Ha: mean(sales|active) != mean(sales|inactive)
  t_res <- t.test(total_sales ~ is_active, data = test_data, alternative = "two.sided")
  
  # Return row
  tibble(
    event_name = evt_name,
    t_statistic = t_res$statistic,
    p_value = t_res$p.value,
    mean_without = t_res$estimate[1], # usually group 0
    mean_with = t_res$estimate[2]     # usually group 1
  )
})

# 4. Process and Display Results
# ------------------------------
final_hypothesis_table <- hypothesis_results %>%
  # Sort by statistical significance (lowest p-value first)
  arrange(p_value) %>%
  mutate(
    # Format p-values for readability (< 0.001, etc.)
    p_value_fmt = ifelse(p_value < 0.001, "< 0.001", round(p_value, 4)),
    t_statistic = round(t_statistic, 2),
    mean_with = round(mean_with, 1),
    mean_without = round(mean_without, 1),
    diff = mean_with - mean_without
  ) %>%
  select(event_name, t_statistic, p_value = p_value_fmt, mean_with, mean_without, diff)

# Display table
knitr::kable(final_hypothesis_table, 
             caption = "Hypothesis Test Results: Impact of Events on Daily Sales (Sorted by Significance)")
```


## 10. Dynamic Harmonic Regression Modeling

We now proceed to fit Dynamic Harmonic Regression models. We use **Fourier terms** to capture the annual seasonality ($period = "year"$) because the data is daily. We test three levels of complexity for the seasonal shape:
* **$K=1$**: Smooth, sine-wave-like annual pattern.
* **$K=5$**: Moderate complexity, capturing broader seasonal shifts.
* **$K=10$**: High complexity, capturing sharper seasonal peaks.

For each $K$, we test four predictor configurations:
1.  **Base**: Fourier terms only.
2.  **Price**: Fourier + `sell_price`.
3.  **Events**: Fourier + Significant Events (`snap_TX`, `halloween`, etc.).
4.  **Full**: Fourier + `sell_price` + Significant Events.

```{r fit_harmonic_regression}
# 1. Define the list of statistically significant predictors
# ---------------------------------------------------------
sig_events <- c(
  "snap_TX", "halloween", "columbusday", "laborday", 
  "independenceday", "fathersday", "easter", "eidalfitr", "christmas"
)

# 2. Define Helper Function to Construct Formulas
# -----------------------------------------------
# CHANGED: 'K' is now uppercase in the function definition to match the calls below.
build_harmonic_formula <- function(K, include_price = FALSE, include_events = FALSE) {
  # Start with Fourier terms
  rhs <- paste0("fourier(period = 'year', K = ", K, ")")
  
  # Add Price
  if(include_price) {
    rhs <- paste(rhs, "+ sell_price")
  }
  
  # Add Events
  if(include_events) {
    # Paste all event names with "+" separator
    events_string <- paste(sig_events, collapse = " + ")
    rhs <- paste(rhs, "+", events_string)
  }
  
  # Return as formula
  as.formula(paste("sales_volume ~", rhs))
}

# 3. Fit Models Grid
# ------------------
# WARNING: This fits 12 ARIMA models for EACH product ID. 
# If you have thousands of products, this step will be computationally intensive.
harmonic_models <- product_tsibble %>%
  model(
    # --- K = 1 (Smooth) ---
    k1_base   = ARIMA(!!build_harmonic_formula(K=1, include_price=FALSE, include_events=FALSE)),
    k1_price  = ARIMA(!!build_harmonic_formula(K=1, include_price=TRUE,  include_events=FALSE)),
    k1_events = ARIMA(!!build_harmonic_formula(K=1, include_price=FALSE, include_events=TRUE)),
    k1_full   = ARIMA(!!build_harmonic_formula(K=1, include_price=TRUE,  include_events=TRUE)),
    
    # --- K = 5 (Medium) ---
    k5_base   = ARIMA(!!build_harmonic_formula(K=5, include_price=FALSE, include_events=FALSE)),
    k5_price  = ARIMA(!!build_harmonic_formula(K=5, include_price=TRUE,  include_events=FALSE)),
    k5_events = ARIMA(!!build_harmonic_formula(K=5, include_price=FALSE, include_events=TRUE)),
    k5_full   = ARIMA(!!build_harmonic_formula(K=5, include_price=TRUE,  include_events=TRUE)),
    
    # --- K = 10 (Complex) ---
    k10_base   = ARIMA(!!build_harmonic_formula(K=10, include_price=FALSE, include_events=FALSE)),
    k10_price  = ARIMA(!!build_harmonic_formula(K=10, include_price=TRUE,  include_events=FALSE)),
    k10_events = ARIMA(!!build_harmonic_formula(K=10, include_price=FALSE, include_events=TRUE)),
    k10_full   = ARIMA(!!build_harmonic_formula(K=10, include_price=TRUE,  include_events=TRUE))
  )

# 4. Compare Model Performance (AICc)
# -----------------------------------
# We look at which model specification wins most frequently across products.
model_rankings <- harmonic_models %>%
  glance() %>%
  group_by(id) %>%
  filter(AICc == min(AICc)) %>%
  ungroup() %>%
  count(.model, name = "count_best_fit") %>%
  arrange(desc(count_best_fit))

# Display the "Leaderboard" of models
knitr::kable(model_rankings, caption = "Count of Products where each Model was the Best Fit (Lowest AICc)")
```


```{r validation_setup}
# 1. Load Validation Data (Ground Truth)
# --------------------------------------
validation_raw <- read_csv("data/sales_test_validation_afcs2025.csv")

# Process Validation Data: Pivot to Long Format
# We must match the structure of the training data exactly.
validation_long <- validation_raw %>%
  pivot_longer(
    cols = starts_with("d_"),
    names_to = "d",
    values_to = "sales_volume"
  ) %>%
  mutate(
    # Extract IDs to ensure key matching
    item_id = str_sub(id, 1, 11),
    store_id_temp = str_sub(id, 13, nchar(id)),
    store_id = str_remove(store_id_temp, "_validation")
  ) %>%
  select(-store_id_temp) %>%
  # Join with Calendar to get actual dates
  left_join(calendar, by = "d") %>%
  # Select only relevant columns for joining/metrics
  select(id, date, sales_volume)

# Convert to Tsibble for comparison
validation_ts <- validation_long %>%
  arrange(id, date) %>%
  as_tsibble(key = id, index = date)


# 2. Prepare Future Regressors for the next 28 days
# -------------------------------------------------
# ARIMA with exogenous regressors needs future values for Price and Events.
future_scenarios <- new_data(product_tsibble, n = 28) %>%
  # Join Calendar to get future events (snap_TX, halloween, etc.) & wm_yr_wk
  left_join(calendar, by = "date") %>%
  # Re-extract item_id and store_id from the 'id' key to join with prices
  mutate(
    item_id = str_sub(id, 1, 11),
    store_id_temp = str_sub(id, 13, nchar(id)),
    store_id = str_remove(store_id_temp, "_validation")
  ) %>%
  # Join Prices to get future sell_price
  left_join(prices_raw, by = c("store_id", "item_id", "wm_yr_wk")) %>%
  # Handle missing prices if any (carry forward last known price)
  group_by(id) %>%
  fill(sell_price, .direction = "downup") %>%
  ungroup()


# 3. Generate Forecasts
# ---------------------
# This generates 28-day forecasts for ALL 12 models for ALL products.
# Note: This step can be memory intensive.
forecasts <- harmonic_models %>%
  forecast(new_data = future_scenarios)

# 4. Compute Accuracy (RMSE) on Validation Set
# --------------------------------------------
# We compare the forecasts against the actual validation_ts
accuracy_results <- forecasts %>%
  accuracy(validation_ts, measures = list(RMSE = RMSE)) %>%
  select(.model, id, RMSE)

# 5. Leaderboard: Which model wins most often on RMSE?
# ----------------------------------------------------
rmse_leaderboard <- accuracy_results %>%
  group_by(id) %>%
  # Find the model with the minimum RMSE for each product
  filter(RMSE == min(RMSE, na.rm = TRUE)) %>%
  ungroup() %>%
  # Count how many times each model was the winner
  count(.model, name = "products_won") %>%
  arrange(desc(products_won))

# 6. Compute Accuracy (MASE) on Validation Set
# --------------------------------------------
# We compare the forecasts against the actual validation_ts
accuracy_results <- forecasts %>%
  accuracy(validation_ts, measures = list(MASE = MASE)) %>%
  select(.model, id, MASE)

# 7. Leaderboard: Which model wins most often on MASE?
# ----------------------------------------------------
MASE_leaderboard <- accuracy_results %>%
  group_by(id) %>%
  # Find the model with the minimum MASE for each product
  filter(MASE == min(MASE, na.rm = TRUE)) %>%
  ungroup() %>%
  # Count how many times each model was the winner
  count(.model, name = "products_won") %>%
  arrange(desc(products_won))

# Display Results
knitr::kable(MASE_leaderboard, caption = "Model Leaderboard: Count of Products Won by MASE on Validation Set (Next 28 Days)")
```